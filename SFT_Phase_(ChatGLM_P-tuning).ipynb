{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e757df80-3567-422e-a394-10969826e780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_chinese\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.20.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge_chinese) (1.14.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2024.5.15)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 53.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.8/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.25.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=be4ec82836102e41d38f1cf0e0bb1ecdb49443ed3f775ac8d40fb2063034baa4\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built jieba\n",
      "Installing collected packages: rouge-chinese, click, joblib, nltk, jieba\n",
      "Successfully installed click-8.1.7 jieba-0.42.1 joblib-1.4.2 nltk-3.8.1 rouge-chinese-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_chinese nltk jieba datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0af41e5-162b-4f19-a386-231d2acb3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM2-6B'...\n",
      "remote: Enumerating objects: 292, done.\u001b[K\n",
      "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 292 (delta 153), reused 132 (delta 128), pack-reused 102\u001b[K\n",
      "Receiving objects: 100% (292/292), 7.36 MiB | 23.93 MiB/s, done.\n",
      "Resolving deltas: 100% (168/168), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/THUDM/ChatGLM2-6B.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174e3040-1cca-4ae6-9129-5846e10ba447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TRL3/ChatGLM2-6B\n"
     ]
    }
   ],
   "source": [
    "%cd ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9158be07-7b4d-4660-9efa-db278a026be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (5.27.2)\n",
      "Collecting transformers==4.30.2\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cpm_kernels\n",
      "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "\u001b[K     |████████████████████████████████| 416 kB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (2.3.1)\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 70.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdtex2html\n",
      "  Downloading mdtex2html-1.3.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.31.0)\n",
      "Collecting sse-starlette\n",
      "  Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting streamlit>=1.24.0\n",
      "  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.15.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (24.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 25.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2024.5.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (11.4.5.107)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (4.12.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (2024.5.0)\n",
      "Requirement already satisfied: triton==2.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=2.0->-r requirements.txt (line 4)) (12.1.105)\n",
      "Collecting urllib3~=2.0\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from gradio->-r requirements.txt (line 5)) (2.0.3)\n",
      "Collecting pillow<11.0,>=8.0\n",
      "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gradio-client==1.0.2\n",
      "  Downloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\n",
      "\u001b[K     |████████████████████████████████| 318 kB 37.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<1.0,>=0.12; sys_platform != \"emscripten\"\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 803 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "Collecting python-multipart>=0.0.9\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.8/dist-packages (from gradio->-r requirements.txt (line 5)) (6.4.0)\n",
      "Collecting ruff>=0.2.2; sys_platform != \"emscripten\"\n",
      "  Downloading ruff-0.5.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 70.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tomlkit==0.12.0\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting semantic-version~=2.0\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting matplotlib~=3.0\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 68.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting orjson~=3.0\n",
      "  Downloading orjson-3.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 73.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.8/dist-packages (from gradio->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.8/dist-packages (from gradio->-r requirements.txt (line 5)) (0.27.0)\n",
      "Collecting pydantic>=2.0\n",
      "  Downloading pydantic-2.8.0-py3-none-any.whl (423 kB)\n",
      "\u001b[K     |████████████████████████████████| 423 kB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn>=0.14.0; sys_platform != \"emscripten\"\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 180 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting altair<6.0,>=4.2.0\n",
      "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[K     |████████████████████████████████| 857 kB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting latex2mathml\n",
      "  Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 247 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 46.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.8)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.8/dist-packages (from sse-starlette->-r requirements.txt (line 9)) (4.4.0)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 66 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 46.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting watchdog<5,>=2.1.5; platform_system != \"Darwin\"\n",
      "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 141 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.24.0->-r requirements.txt (line 10)) (6.4)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.24.0->-r requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.24.0->-r requirements.txt (line 10)) (16.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.24.0->-r requirements.txt (line 10)) (8.1.7)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Downloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2019.11.28)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0->-r requirements.txt (line 4)) (12.5.82)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=2.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 5)) (2024.1)\n",
      "Collecting websockets<12.0,>=10.0\n",
      "  Downloading websockets-11.0.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting email_validator>=2.0.0\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1\n",
      "  Downloading ujson-5.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 195 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi-cli>=0.0.2\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources<7.0,>=1.3->gradio->-r requirements.txt (line 5)) (3.19.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 42.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 45.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 33.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 5)) (1.0.5)\n",
      "Collecting pydantic-core==2.20.0\n",
      "  Downloading pydantic_core-2.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 62.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn>=0.14.0; sys_platform != \"emscripten\"->gradio->-r requirements.txt (line 5)) (0.14.0)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 739 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (4.22.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown->mdtex2html->-r requirements.txt (line 6)) (7.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from anyio->sse-starlette->-r requirements.txt (line 9)) (1.2.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 259 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<14,>=10.14.0->streamlit>=1.24.0->-r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from rich<14,>=10.14.0->streamlit>=1.24.0->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 5)) (1.14.0)\n",
      "Collecting dnspython>=2.0.0\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 45.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (1.3.10)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (23.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 5)) (2023.12.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.24.0->-r requirements.txt (line 10)) (0.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5586 sha256=ee48801814b3ba1c84ac04de5d04c685aa5bf77634cb8e13abc5a1108eb84867\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/2c/52/5e817a3b09f1081927f4c27751360f0189ae6957c99b6b132a\n",
      "Successfully built ffmpy\n",
      "\u001b[31mERROR: trl 0.9.4 has requirement transformers>=4.31.0, but you'll have transformers 4.30.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers, cpm-kernels, urllib3, pillow, websockets, gradio-client, shellingham, typer, orjson, python-multipart, dnspython, email-validator, uvicorn, ujson, starlette, fastapi-cli, pydantic-core, annotated-types, pydantic, fastapi, ffmpy, ruff, tomlkit, semantic-version, aiofiles, fonttools, pyparsing, cycler, contourpy, kiwisolver, matplotlib, toolz, altair, pydub, gradio, latex2mathml, markdown, mdtex2html, sse-starlette, smmap, gitdb, gitpython, cachetools, watchdog, toml, tenacity, pydeck, blinker, streamlit\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.3\n",
      "    Uninstalling transformers-4.42.3:\n",
      "      Successfully uninstalled transformers-4.42.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Not uninstalling urllib3 at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'urllib3'. No files were found to uninstall.\n",
      "Successfully installed aiofiles-23.2.1 altair-5.3.0 annotated-types-0.7.0 blinker-1.8.2 cachetools-5.3.3 contourpy-1.1.1 cpm-kernels-1.0.11 cycler-0.12.1 dnspython-2.6.1 email-validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fonttools-4.53.0 gitdb-4.0.11 gitpython-3.1.43 gradio-4.37.2 gradio-client-1.0.2 kiwisolver-1.4.5 latex2mathml-3.77.0 markdown-3.6 matplotlib-3.7.5 mdtex2html-1.3.0 orjson-3.10.6 pillow-10.4.0 pydantic-2.8.0 pydantic-core-2.20.0 pydeck-0.9.1 pydub-0.25.1 pyparsing-3.1.2 python-multipart-0.0.9 ruff-0.5.0 semantic-version-2.10.0 shellingham-1.5.4 smmap-5.0.1 sse-starlette-2.1.2 starlette-0.37.2 streamlit-1.36.0 tenacity-8.4.2 tokenizers-0.13.3 toml-0.10.2 tomlkit-0.12.0 toolz-0.12.1 transformers-4.30.2 typer-0.12.3 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.1 watchdog-4.0.1 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9e95ed-7e03-4241-8db0-a847bb84e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: accelerate in /usr/local/lib/python3.8/dist-packages (0.31.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied, skipping upgrade: triton==2.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.10.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied, skipping upgrade: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->accelerate) (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58228e07-225b-4c69-86bc-f70e37668bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TRL3/ChatGLM2-6B/ptuning\n"
     ]
    }
   ],
   "source": [
    "%cd ptuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0308ef6d-d324-4466-94c8-fc97f3091847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03/2024 09:18:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/03/2024 09:18:11 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.015,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/Final_PTuning_ChatGLM_SFT_Model,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=4000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=SFT_Model_ChatGLM,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=SFT_Model_ChatGLM,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Generating train split: 25140 examples [00:00, 290066.83 examples/s]\n",
      "Generating validation split: 8670 examples [00:00, 564046.09 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|██████████████████████████| 1.17k/1.17k [00:00<00:00, 138kB/s]\n",
      "[INFO|configuration_utils.py:669] 2024-07-03 09:18:11,780 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm2-6b-int4/snapshots/66ecaf1db3a5085714e133357ea4824b69698743/config.json\n",
      "[INFO|configuration_utils.py:669] 2024-07-03 09:18:11,929 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm2-6b-int4/snapshots/66ecaf1db3a5085714e133357ea4824b69698743/config.json\n",
      "[INFO|configuration_utils.py:725] 2024-07-03 09:18:11,932 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm2-6b-int4\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm2-6b-int4--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm2-6b-int4--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm2-6b-int4--modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 4,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 32768,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████████| 243/243 [00:00<00:00, 66.0kB/s]\n",
      "tokenization_chatglm.py: 100%|█████████████| 10.1k/10.1k [00:00<00:00, 2.74MB/s]\n",
      "[WARNING|dynamic_module_utils.py:324] 2024-07-03 09:18:12,303 >> A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b-int4:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "tokenizer.model: 100%|█████████████████████| 1.02M/1.02M [00:00<00:00, 12.2MB/s]\n",
      "[INFO|tokenization_utils_base.py:1823] 2024-07-03 09:18:12,792 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm2-6b-int4/snapshots/66ecaf1db3a5085714e133357ea4824b69698743/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2024-07-03 09:18:12,793 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2024-07-03 09:18:12,793 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2024-07-03 09:18:12,793 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm2-6b-int4/snapshots/66ecaf1db3a5085714e133357ea4824b69698743/tokenizer_config.json\n",
      "pytorch_model.bin: 100%|████████████████████| 3.92G/3.92G [00:36<00:00, 107MB/s]\n",
      "[INFO|modeling_utils.py:2578] 2024-07-03 09:18:50,213 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm2-6b-int4/snapshots/66ecaf1db3a5085714e133357ea4824b69698743/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:577] 2024-07-03 09:18:57,863 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3295] 2024-07-03 09:19:05,873 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
      "\n",
      "[WARNING|modeling_utils.py:3297] 2024-07-03 09:19:05,873 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm2-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:2927] 2024-07-03 09:19:05,955 >> Generation config file not found, using a generation config created from the model config.\n",
      "Running tokenizer on train dataset: 100%|█| 25140/25140 [00:27<00:00, 926.37 exa\n",
      "input_ids [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 31010, 30959, 16937, 33232, 55001, 54901, 54880, 56313, 54927, 54957, 31010, 31045, 55033, 54838, 30939, 30967, 30972, 34054, 54973, 54589, 55151, 30966, 30941, 30940, 55773, 55416, 55397, 57953, 30939, 30939, 30941, 30966, 31123, 30939, 30939, 30941, 30972, 31123, 30939, 30939, 30941, 30981, 13, 13, 55437, 31211, 30910, 54835, 39020, 56540, 55214, 54973, 54589, 55151, 31820, 32131, 32986, 33075, 55370, 33897, 33897, 54835, 54536, 38627, 32507, 31123, 36310, 31627, 54567, 44981, 33897, 33897, 54546, 32663, 32192, 54603, 54973, 34569, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "inputs [Round 1]\n",
      "\n",
      "问：#WTT冠军赛布达佩斯站# 男单1/4决赛林高远3-0宇田幸矢11-3，11-4，11-7\n",
      "\n",
      "答： 别把我帅死林高远一直这么坚定下去吧！！！！别有太大压力，战胜自己就够了！！！！我永远相信小林将军\n",
      "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 54835, 39020, 56540, 55214, 54973, 54589, 55151, 31820, 32131, 32986, 33075, 55370, 33897, 33897, 54835, 54536, 38627, 32507, 31123, 36310, 31627, 54567, 44981, 33897, 33897, 54546, 32663, 32192, 54603, 54973, 34569, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "labels 别把我帅死林高远一直这么坚定下去吧！！！！别有太大压力，战胜自己就够了！！！！我永远相信小林将军\n",
      "07/03/2024 09:19:33 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:577] 2024-07-03 09:19:36,945 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2024-07-03 09:19:37,747 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2024-07-03 09:19:37,747 >>   Num examples = 25,140\n",
      "[INFO|trainer.py:1788] 2024-07-03 09:19:37,747 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1789] 2024-07-03 09:19:37,747 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2024-07-03 09:19:37,748 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1791] 2024-07-03 09:19:37,748 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1792] 2024-07-03 09:19:37,748 >>   Total optimization steps = 4,000\n",
      "[INFO|trainer.py:1793] 2024-07-03 09:19:37,749 >>   Number of trainable parameters = 1,835,008\n",
      "  0%|                                                  | 0/4000 [00:00<?, ?it/s]07/03/2024 09:19:38 - WARNING - transformers_modules.THUDM.chatglm2-6b-int4.66ecaf1db3a5085714e133357ea4824b69698743.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 6.8453, 'learning_rate': 0.0149625, 'epoch': 0.0}                      \n",
      "{'loss': 6.0498, 'learning_rate': 0.014924999999999999, 'epoch': 0.0}           \n",
      "{'loss': 5.4843, 'learning_rate': 0.0148875, 'epoch': 0.0}                      \n",
      "{'loss': 5.6433, 'learning_rate': 0.014849999999999999, 'epoch': 0.01}          \n",
      "{'loss': 5.2511, 'learning_rate': 0.0148125, 'epoch': 0.01}                     \n",
      "{'loss': 5.04, 'learning_rate': 0.014775, 'epoch': 0.01}                        \n",
      "{'loss': 4.9666, 'learning_rate': 0.0147375, 'epoch': 0.01}                     \n",
      "{'loss': 5.1644, 'learning_rate': 0.0147, 'epoch': 0.01}                        \n",
      "{'loss': 4.7396, 'learning_rate': 0.0146625, 'epoch': 0.01}                     \n",
      "{'loss': 4.9953, 'learning_rate': 0.014624999999999999, 'epoch': 0.02}          \n",
      "  2%|▉                                     | 100/4000 [02:27<1:34:28,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:22:05,630 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:22:05,631 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-100/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:22:05,648 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-100/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:22:05,648 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:22:05,649 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-100/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.4703, 'learning_rate': 0.0145875, 'epoch': 0.02}                     \n",
      "{'loss': 4.852, 'learning_rate': 0.014549999999999999, 'epoch': 0.02}           \n",
      "{'loss': 4.7424, 'learning_rate': 0.0145125, 'epoch': 0.02}                     \n",
      "{'loss': 4.8815, 'learning_rate': 0.014474999999999998, 'epoch': 0.02}          \n",
      "{'loss': 4.6644, 'learning_rate': 0.014437499999999999, 'epoch': 0.02}          \n",
      "{'loss': 4.6464, 'learning_rate': 0.0144, 'epoch': 0.03}                        \n",
      "{'loss': 5.0753, 'learning_rate': 0.0143625, 'epoch': 0.03}                     \n",
      "{'loss': 4.7357, 'learning_rate': 0.014325, 'epoch': 0.03}                      \n",
      "{'loss': 4.8022, 'learning_rate': 0.0142875, 'epoch': 0.03}                     \n",
      "{'loss': 4.6094, 'learning_rate': 0.014249999999999999, 'epoch': 0.03}          \n",
      "  5%|█▉                                    | 200/4000 [04:53<1:32:42,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:24:31,390 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-200/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:24:31,390 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-200/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:24:31,400 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-200/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:24:31,400 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:24:31,400 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-200/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.5679, 'learning_rate': 0.0142125, 'epoch': 0.03}                     \n",
      "{'loss': 4.4904, 'learning_rate': 0.014174999999999998, 'epoch': 0.04}          \n",
      "{'loss': 4.5622, 'learning_rate': 0.014137499999999999, 'epoch': 0.04}          \n",
      "{'loss': 4.4159, 'learning_rate': 0.014099999999999998, 'epoch': 0.04}          \n",
      "{'loss': 4.8525, 'learning_rate': 0.014062499999999999, 'epoch': 0.04}          \n",
      "{'loss': 4.9306, 'learning_rate': 0.014025000000000001, 'epoch': 0.04}          \n",
      "{'loss': 4.6505, 'learning_rate': 0.0139875, 'epoch': 0.04}                     \n",
      "{'loss': 5.1604, 'learning_rate': 0.01395, 'epoch': 0.04}                       \n",
      "{'loss': 4.2098, 'learning_rate': 0.0139125, 'epoch': 0.05}                     \n",
      "{'loss': 4.6984, 'learning_rate': 0.013875, 'epoch': 0.05}                      \n",
      "  8%|██▊                                   | 300/4000 [07:19<1:29:39,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:26:57,182 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-300/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:26:57,182 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-300/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:26:57,197 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-300/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:26:57,198 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:26:57,198 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-300/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2881, 'learning_rate': 0.013837499999999999, 'epoch': 0.05}          \n",
      "{'loss': 4.6407, 'learning_rate': 0.0138, 'epoch': 0.05}                        \n",
      "{'loss': 4.5209, 'learning_rate': 0.013762499999999999, 'epoch': 0.05}          \n",
      "{'loss': 4.8038, 'learning_rate': 0.013725, 'epoch': 0.05}                      \n",
      "{'loss': 4.3989, 'learning_rate': 0.0136875, 'epoch': 0.06}                     \n",
      "{'loss': 4.543, 'learning_rate': 0.01365, 'epoch': 0.06}                        \n",
      "{'loss': 4.6841, 'learning_rate': 0.0136125, 'epoch': 0.06}                     \n",
      "{'loss': 4.4335, 'learning_rate': 0.013575, 'epoch': 0.06}                      \n",
      "{'loss': 4.37, 'learning_rate': 0.0135375, 'epoch': 0.06}                       \n",
      "{'loss': 4.4729, 'learning_rate': 0.0135, 'epoch': 0.06}                        \n",
      " 10%|███▊                                  | 400/4000 [09:45<1:27:17,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:29:22,943 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-400/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:29:22,943 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-400/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:29:22,958 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-400/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:29:22,959 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:29:22,959 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-400/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.4779, 'learning_rate': 0.013462499999999999, 'epoch': 0.07}          \n",
      "{'loss': 4.7846, 'learning_rate': 0.013425, 'epoch': 0.07}                      \n",
      "{'loss': 4.4435, 'learning_rate': 0.013387499999999998, 'epoch': 0.07}          \n",
      "{'loss': 4.2452, 'learning_rate': 0.013349999999999999, 'epoch': 0.07}          \n",
      "{'loss': 4.7467, 'learning_rate': 0.0133125, 'epoch': 0.07}                     \n",
      "{'loss': 4.6479, 'learning_rate': 0.013275, 'epoch': 0.07}                      \n",
      "{'loss': 4.7687, 'learning_rate': 0.0132375, 'epoch': 0.07}                     \n",
      "{'loss': 4.8555, 'learning_rate': 0.0132, 'epoch': 0.08}                        \n",
      "{'loss': 5.0677, 'learning_rate': 0.013162499999999999, 'epoch': 0.08}          \n",
      "{'loss': 4.3941, 'learning_rate': 0.013125, 'epoch': 0.08}                      \n",
      " 12%|████▊                                 | 500/4000 [12:11<1:24:35,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:31:48,853 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:31:48,854 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:31:48,863 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:31:48,864 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:31:48,864 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-500/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.7733, 'learning_rate': 0.0130875, 'epoch': 0.08}                     \n",
      "{'loss': 4.8532, 'learning_rate': 0.013049999999999999, 'epoch': 0.08}          \n",
      "{'loss': 4.4654, 'learning_rate': 0.0130125, 'epoch': 0.08}                     \n",
      "{'loss': 4.1729, 'learning_rate': 0.012974999999999999, 'epoch': 0.09}          \n",
      "{'loss': 4.391, 'learning_rate': 0.0129375, 'epoch': 0.09}                      \n",
      "{'loss': 4.6863, 'learning_rate': 0.0129, 'epoch': 0.09}                        \n",
      "{'loss': 4.4892, 'learning_rate': 0.0128625, 'epoch': 0.09}                     \n",
      "{'loss': 4.5623, 'learning_rate': 0.012825, 'epoch': 0.09}                      \n",
      "{'loss': 4.5782, 'learning_rate': 0.0127875, 'epoch': 0.09}                     \n",
      "{'loss': 4.4822, 'learning_rate': 0.01275, 'epoch': 0.1}                        \n",
      " 15%|█████▋                                | 600/4000 [14:36<1:22:46,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:34:14,301 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-600/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:34:14,301 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-600/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:34:14,315 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-600/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:34:14,315 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:34:14,316 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-600/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.3159, 'learning_rate': 0.0127125, 'epoch': 0.1}                      \n",
      "{'loss': 4.0687, 'learning_rate': 0.012674999999999999, 'epoch': 0.1}           \n",
      "{'loss': 4.366, 'learning_rate': 0.0126375, 'epoch': 0.1}                       \n",
      "{'loss': 4.5618, 'learning_rate': 0.012599999999999998, 'epoch': 0.1}           \n",
      "{'loss': 4.1283, 'learning_rate': 0.012562499999999999, 'epoch': 0.1}           \n",
      "{'loss': 4.7448, 'learning_rate': 0.012525, 'epoch': 0.11}                      \n",
      "{'loss': 4.7464, 'learning_rate': 0.0124875, 'epoch': 0.11}                     \n",
      "{'loss': 4.0351, 'learning_rate': 0.01245, 'epoch': 0.11}                       \n",
      "{'loss': 4.6131, 'learning_rate': 0.0124125, 'epoch': 0.11}                     \n",
      "{'loss': 4.1214, 'learning_rate': 0.012374999999999999, 'epoch': 0.11}          \n",
      " 18%|██████▋                               | 700/4000 [17:02<1:20:11,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:36:40,203 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-700/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:36:40,204 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-700/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:36:40,217 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-700/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:36:40,218 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:36:40,218 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-700/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.5193, 'learning_rate': 0.0123375, 'epoch': 0.11}                     \n",
      "{'loss': 4.2749, 'learning_rate': 0.012299999999999998, 'epoch': 0.11}          \n",
      "{'loss': 4.7822, 'learning_rate': 0.012262499999999999, 'epoch': 0.12}          \n",
      "{'loss': 4.3633, 'learning_rate': 0.012224999999999998, 'epoch': 0.12}          \n",
      "{'loss': 4.2284, 'learning_rate': 0.0121875, 'epoch': 0.12}                     \n",
      "{'loss': 4.4918, 'learning_rate': 0.012150000000000001, 'epoch': 0.12}          \n",
      "{'loss': 4.4166, 'learning_rate': 0.0121125, 'epoch': 0.12}                     \n",
      "{'loss': 4.3203, 'learning_rate': 0.012075, 'epoch': 0.12}                      \n",
      "{'loss': 4.6738, 'learning_rate': 0.0120375, 'epoch': 0.13}                     \n",
      "{'loss': 4.8146, 'learning_rate': 0.012, 'epoch': 0.13}                         \n",
      " 20%|███████▌                              | 800/4000 [19:27<1:17:07,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:39:05,698 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-800/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:39:05,699 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-800/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:39:05,708 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-800/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:39:05,708 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:39:05,709 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-800/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.104, 'learning_rate': 0.0119625, 'epoch': 0.13}                      \n",
      "{'loss': 4.5776, 'learning_rate': 0.011925, 'epoch': 0.13}                      \n",
      "{'loss': 4.3342, 'learning_rate': 0.011887499999999999, 'epoch': 0.13}          \n",
      "{'loss': 4.0693, 'learning_rate': 0.01185, 'epoch': 0.13}                       \n",
      "{'loss': 4.3709, 'learning_rate': 0.0118125, 'epoch': 0.14}                     \n",
      "{'loss': 4.5704, 'learning_rate': 0.011775, 'epoch': 0.14}                      \n",
      "{'loss': 4.7057, 'learning_rate': 0.0117375, 'epoch': 0.14}                     \n",
      "{'loss': 4.3438, 'learning_rate': 0.0117, 'epoch': 0.14}                        \n",
      "{'loss': 4.3315, 'learning_rate': 0.0116625, 'epoch': 0.14}                     \n",
      "{'loss': 4.5336, 'learning_rate': 0.011625, 'epoch': 0.14}                      \n",
      " 22%|████████▌                             | 900/4000 [21:53<1:14:55,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:41:31,511 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-900/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:41:31,512 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-900/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:41:31,521 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-900/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:41:31,522 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:41:31,522 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-900/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.3633, 'learning_rate': 0.011587499999999999, 'epoch': 0.14}          \n",
      "{'loss': 4.688, 'learning_rate': 0.01155, 'epoch': 0.15}                        \n",
      "{'loss': 4.2703, 'learning_rate': 0.011512499999999998, 'epoch': 0.15}          \n",
      "{'loss': 4.1249, 'learning_rate': 0.011474999999999999, 'epoch': 0.15}          \n",
      "{'loss': 4.7769, 'learning_rate': 0.0114375, 'epoch': 0.15}                     \n",
      "{'loss': 4.9771, 'learning_rate': 0.0114, 'epoch': 0.15}                        \n",
      "{'loss': 4.0525, 'learning_rate': 0.0113625, 'epoch': 0.15}                     \n",
      "{'loss': 4.6248, 'learning_rate': 0.011325, 'epoch': 0.16}                      \n",
      "{'loss': 4.3242, 'learning_rate': 0.011287499999999999, 'epoch': 0.16}          \n",
      "{'loss': 4.3595, 'learning_rate': 0.01125, 'epoch': 0.16}                       \n",
      " 25%|█████████▎                           | 1000/4000 [24:19<1:13:07,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:43:57,138 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:43:57,139 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:43:57,152 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:43:57,152 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:43:57,152 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1000/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2003, 'learning_rate': 0.0112125, 'epoch': 0.16}                     \n",
      "{'loss': 4.2833, 'learning_rate': 0.011175, 'epoch': 0.16}                      \n",
      "{'loss': 4.0209, 'learning_rate': 0.0111375, 'epoch': 0.16}                     \n",
      "{'loss': 4.3899, 'learning_rate': 0.011099999999999999, 'epoch': 0.17}          \n",
      "{'loss': 4.4837, 'learning_rate': 0.0110625, 'epoch': 0.17}                     \n",
      "{'loss': 4.3481, 'learning_rate': 0.011025, 'epoch': 0.17}                      \n",
      "{'loss': 4.6476, 'learning_rate': 0.0109875, 'epoch': 0.17}                     \n",
      "{'loss': 4.3747, 'learning_rate': 0.01095, 'epoch': 0.17}                       \n",
      "{'loss': 4.3281, 'learning_rate': 0.0109125, 'epoch': 0.17}                     \n",
      "{'loss': 4.6708, 'learning_rate': 0.010875, 'epoch': 0.18}                      \n",
      " 28%|██████████▏                          | 1100/4000 [26:44<1:10:09,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:46:22,788 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1100/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:46:22,789 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1100/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:46:22,802 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1100/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:46:22,803 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:46:22,803 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1100/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.4219, 'learning_rate': 0.0108375, 'epoch': 0.18}                     \n",
      "{'loss': 4.0885, 'learning_rate': 0.010799999999999999, 'epoch': 0.18}          \n",
      "{'loss': 4.4862, 'learning_rate': 0.0107625, 'epoch': 0.18}                     \n",
      "{'loss': 4.2573, 'learning_rate': 0.010724999999999998, 'epoch': 0.18}          \n",
      "{'loss': 4.512, 'learning_rate': 0.010687499999999999, 'epoch': 0.18}           \n",
      "{'loss': 4.8207, 'learning_rate': 0.01065, 'epoch': 0.18}                       \n",
      "{'loss': 4.0138, 'learning_rate': 0.0106125, 'epoch': 0.19}                     \n",
      "{'loss': 4.4377, 'learning_rate': 0.010575, 'epoch': 0.19}                      \n",
      "{'loss': 4.2091, 'learning_rate': 0.0105375, 'epoch': 0.19}                     \n",
      "{'loss': 4.2612, 'learning_rate': 0.010499999999999999, 'epoch': 0.19}          \n",
      " 30%|███████████                          | 1200/4000 [29:11<1:07:41,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:48:49,138 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1200/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:48:49,139 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1200/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:48:49,149 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1200/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:48:49,150 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:48:49,150 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1200/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.9903, 'learning_rate': 0.0104625, 'epoch': 0.19}                     \n",
      "{'loss': 4.3597, 'learning_rate': 0.010424999999999999, 'epoch': 0.19}          \n",
      "{'loss': 4.1683, 'learning_rate': 0.0103875, 'epoch': 0.2}                      \n",
      "{'loss': 4.3702, 'learning_rate': 0.010349999999999998, 'epoch': 0.2}           \n",
      "{'loss': 4.1781, 'learning_rate': 0.010312499999999999, 'epoch': 0.2}           \n",
      "{'loss': 4.6572, 'learning_rate': 0.010275000000000001, 'epoch': 0.2}           \n",
      "{'loss': 4.4105, 'learning_rate': 0.0102375, 'epoch': 0.2}                      \n",
      "{'loss': 4.3832, 'learning_rate': 0.0102, 'epoch': 0.2}                         \n",
      "{'loss': 4.2142, 'learning_rate': 0.0101625, 'epoch': 0.21}                     \n",
      "{'loss': 4.4664, 'learning_rate': 0.010125, 'epoch': 0.21}                      \n",
      " 32%|████████████                         | 1300/4000 [31:36<1:05:16,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:51:14,599 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1300/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:51:14,599 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1300/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:51:14,609 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1300/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:51:14,609 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:51:14,609 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1300/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2587, 'learning_rate': 0.0100875, 'epoch': 0.21}                     \n",
      "{'loss': 4.4647, 'learning_rate': 0.01005, 'epoch': 0.21}                       \n",
      "{'loss': 4.4151, 'learning_rate': 0.010012499999999999, 'epoch': 0.21}          \n",
      "{'loss': 4.2182, 'learning_rate': 0.009975, 'epoch': 0.21}                      \n",
      "{'loss': 4.2133, 'learning_rate': 0.0099375, 'epoch': 0.21}                     \n",
      "{'loss': 4.3002, 'learning_rate': 0.0099, 'epoch': 0.22}                        \n",
      "{'loss': 4.4577, 'learning_rate': 0.0098625, 'epoch': 0.22}                     \n",
      "{'loss': 4.5514, 'learning_rate': 0.009825, 'epoch': 0.22}                      \n",
      "{'loss': 4.3521, 'learning_rate': 0.0097875, 'epoch': 0.22}                     \n",
      "{'loss': 4.5182, 'learning_rate': 0.00975, 'epoch': 0.22}                       \n",
      " 35%|████████████▉                        | 1400/4000 [34:02<1:02:52,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:53:40,251 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1400/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:53:40,251 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1400/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:53:40,261 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1400/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:53:40,262 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:53:40,262 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1400/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2794, 'learning_rate': 0.009712499999999999, 'epoch': 0.22}          \n",
      "{'loss': 4.4376, 'learning_rate': 0.009675, 'epoch': 0.23}                      \n",
      "{'loss': 4.3247, 'learning_rate': 0.009637499999999998, 'epoch': 0.23}          \n",
      "{'loss': 4.1946, 'learning_rate': 0.0096, 'epoch': 0.23}                        \n",
      "{'loss': 4.2539, 'learning_rate': 0.0095625, 'epoch': 0.23}                     \n",
      "{'loss': 4.2779, 'learning_rate': 0.009525, 'epoch': 0.23}                      \n",
      "{'loss': 4.1956, 'learning_rate': 0.0094875, 'epoch': 0.23}                     \n",
      "{'loss': 4.0089, 'learning_rate': 0.00945, 'epoch': 0.24}                       \n",
      "{'loss': 4.149, 'learning_rate': 0.009412499999999999, 'epoch': 0.24}           \n",
      "{'loss': 3.9787, 'learning_rate': 0.009375, 'epoch': 0.24}                      \n",
      " 38%|█████████████▉                       | 1500/4000 [36:28<1:00:25,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:56:05,840 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:56:05,840 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:56:05,851 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:56:05,852 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:56:05,852 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1500/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.314, 'learning_rate': 0.0093375, 'epoch': 0.24}                      \n",
      "{'loss': 4.1901, 'learning_rate': 0.0093, 'epoch': 0.24}                        \n",
      "{'loss': 4.602, 'learning_rate': 0.0092625, 'epoch': 0.24}                      \n",
      "{'loss': 4.3454, 'learning_rate': 0.009224999999999999, 'epoch': 0.25}          \n",
      "{'loss': 4.2958, 'learning_rate': 0.0091875, 'epoch': 0.25}                     \n",
      "{'loss': 4.366, 'learning_rate': 0.00915, 'epoch': 0.25}                        \n",
      "{'loss': 4.7428, 'learning_rate': 0.0091125, 'epoch': 0.25}                     \n",
      "{'loss': 4.5818, 'learning_rate': 0.009075, 'epoch': 0.25}                      \n",
      "{'loss': 4.304, 'learning_rate': 0.0090375, 'epoch': 0.25}                      \n",
      "{'loss': 4.439, 'learning_rate': 0.009, 'epoch': 0.25}                          \n",
      " 40%|███████████████▌                       | 1600/4000 [38:53<58:12,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 09:58:31,509 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1600/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 09:58:31,510 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1600/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 09:58:31,519 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1600/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 09:58:31,519 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 09:58:31,520 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1600/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2201, 'learning_rate': 0.0089625, 'epoch': 0.26}                     \n",
      "{'loss': 4.2423, 'learning_rate': 0.008924999999999999, 'epoch': 0.26}          \n",
      "{'loss': 4.5932, 'learning_rate': 0.0088875, 'epoch': 0.26}                     \n",
      "{'loss': 4.4439, 'learning_rate': 0.008849999999999998, 'epoch': 0.26}          \n",
      "{'loss': 4.23, 'learning_rate': 0.0088125, 'epoch': 0.26}                       \n",
      "{'loss': 4.3622, 'learning_rate': 0.008775, 'epoch': 0.26}                      \n",
      "{'loss': 4.4166, 'learning_rate': 0.0087375, 'epoch': 0.27}                     \n",
      "{'loss': 4.3178, 'learning_rate': 0.0087, 'epoch': 0.27}                        \n",
      "{'loss': 4.2972, 'learning_rate': 0.0086625, 'epoch': 0.27}                     \n",
      "{'loss': 4.288, 'learning_rate': 0.008624999999999999, 'epoch': 0.27}           \n",
      " 42%|████████████████▌                      | 1700/4000 [41:19<55:36,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:00:57,203 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1700/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:00:57,204 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1700/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:00:57,214 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1700/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:00:57,214 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:00:57,214 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1700/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.054, 'learning_rate': 0.0085875, 'epoch': 0.27}                      \n",
      "{'loss': 4.1968, 'learning_rate': 0.008549999999999999, 'epoch': 0.27}          \n",
      "{'loss': 4.2514, 'learning_rate': 0.0085125, 'epoch': 0.28}                     \n",
      "{'loss': 3.9531, 'learning_rate': 0.008474999999999998, 'epoch': 0.28}          \n",
      "{'loss': 4.4065, 'learning_rate': 0.0084375, 'epoch': 0.28}                     \n",
      "{'loss': 4.4464, 'learning_rate': 0.008400000000000001, 'epoch': 0.28}          \n",
      "{'loss': 4.3087, 'learning_rate': 0.0083625, 'epoch': 0.28}                     \n",
      "{'loss': 4.0553, 'learning_rate': 0.008325, 'epoch': 0.28}                      \n",
      "{'loss': 4.4987, 'learning_rate': 0.0082875, 'epoch': 0.28}                     \n",
      "{'loss': 4.1172, 'learning_rate': 0.00825, 'epoch': 0.29}                       \n",
      " 45%|█████████████████▌                     | 1800/4000 [43:45<53:28,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:03:22,945 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1800/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:03:22,945 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1800/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:03:22,955 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1800/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:03:22,955 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:03:22,956 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1800/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2586, 'learning_rate': 0.0082125, 'epoch': 0.29}                     \n",
      "{'loss': 4.453, 'learning_rate': 0.008175, 'epoch': 0.29}                       \n",
      "{'loss': 4.7064, 'learning_rate': 0.008137499999999999, 'epoch': 0.29}          \n",
      "{'loss': 4.252, 'learning_rate': 0.0081, 'epoch': 0.29}                         \n",
      "{'loss': 4.2652, 'learning_rate': 0.0080625, 'epoch': 0.29}                     \n",
      "{'loss': 4.2219, 'learning_rate': 0.008025000000000001, 'epoch': 0.3}           \n",
      "{'loss': 4.3366, 'learning_rate': 0.0079875, 'epoch': 0.3}                      \n",
      "{'loss': 4.368, 'learning_rate': 0.00795, 'epoch': 0.3}                         \n",
      "{'loss': 4.0116, 'learning_rate': 0.0079125, 'epoch': 0.3}                      \n",
      "{'loss': 4.2326, 'learning_rate': 0.007875, 'epoch': 0.3}                       \n",
      " 48%|██████████████████▌                    | 1900/4000 [46:11<51:04,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:05:49,151 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1900/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:05:49,152 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-1900/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:05:49,165 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-1900/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:05:49,165 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-1900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:05:49,166 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-1900/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.3515, 'learning_rate': 0.007837499999999999, 'epoch': 0.3}           \n",
      "{'loss': 4.224, 'learning_rate': 0.0078, 'epoch': 0.31}                         \n",
      "{'loss': 4.0004, 'learning_rate': 0.0077624999999999994, 'epoch': 0.31}         \n",
      "{'loss': 3.9523, 'learning_rate': 0.007725, 'epoch': 0.31}                      \n",
      "{'loss': 4.0762, 'learning_rate': 0.007687499999999999, 'epoch': 0.31}          \n",
      "{'loss': 4.2526, 'learning_rate': 0.00765, 'epoch': 0.31}                       \n",
      "{'loss': 4.3755, 'learning_rate': 0.007612499999999999, 'epoch': 0.31}          \n",
      "{'loss': 4.3759, 'learning_rate': 0.007575, 'epoch': 0.32}                      \n",
      "{'loss': 4.0749, 'learning_rate': 0.007537499999999999, 'epoch': 0.32}          \n",
      "{'loss': 4.3809, 'learning_rate': 0.0075, 'epoch': 0.32}                        \n",
      " 50%|███████████████████▌                   | 2000/4000 [48:37<48:34,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:08:15,440 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:08:15,441 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:08:15,454 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:08:15,455 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:08:15,455 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2000/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.1002, 'learning_rate': 0.0074624999999999995, 'epoch': 0.32}         \n",
      "{'loss': 4.5043, 'learning_rate': 0.007424999999999999, 'epoch': 0.32}          \n",
      "{'loss': 4.3063, 'learning_rate': 0.0073875, 'epoch': 0.32}                     \n",
      "{'loss': 4.0816, 'learning_rate': 0.00735, 'epoch': 0.32}                       \n",
      "{'loss': 4.0096, 'learning_rate': 0.0073124999999999996, 'epoch': 0.33}         \n",
      "{'loss': 4.4995, 'learning_rate': 0.007274999999999999, 'epoch': 0.33}          \n",
      "{'loss': 4.395, 'learning_rate': 0.007237499999999999, 'epoch': 0.33}           \n",
      "{'loss': 3.8181, 'learning_rate': 0.0072, 'epoch': 0.33}                        \n",
      "{'loss': 4.0996, 'learning_rate': 0.0071625, 'epoch': 0.33}                     \n",
      "{'loss': 4.1223, 'learning_rate': 0.007124999999999999, 'epoch': 0.33}          \n",
      " 52%|████████████████████▍                  | 2100/4000 [51:03<45:57,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:10:41,113 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2100/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:10:41,113 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2100/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:10:41,123 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2100/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:10:41,124 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:10:41,124 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2100/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2325, 'learning_rate': 0.007087499999999999, 'epoch': 0.34}          \n",
      "{'loss': 4.2306, 'learning_rate': 0.007049999999999999, 'epoch': 0.34}          \n",
      "{'loss': 4.1474, 'learning_rate': 0.0070125000000000005, 'epoch': 0.34}         \n",
      "{'loss': 4.2347, 'learning_rate': 0.006975, 'epoch': 0.34}                      \n",
      "{'loss': 4.2354, 'learning_rate': 0.0069375, 'epoch': 0.34}                     \n",
      "{'loss': 4.308, 'learning_rate': 0.005362499999999999, 'epoch': 0.41}           \n",
      "{'loss': 4.5878, 'learning_rate': 0.005325, 'epoch': 0.41}                      \n",
      "{'loss': 4.3298, 'learning_rate': 0.0052875, 'epoch': 0.41}                     \n",
      "{'loss': 4.388, 'learning_rate': 0.0052499999999999995, 'epoch': 0.41}          \n",
      " 65%|████████████████████████             | 2600/4000 [1:03:12<33:47,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:22:50,163 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2600/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:22:50,164 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2600/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:22:50,174 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2600/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:22:50,175 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:22:50,175 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2600/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.3137, 'learning_rate': 0.005212499999999999, 'epoch': 0.42}          \n",
      "{'loss': 4.1759, 'learning_rate': 0.005174999999999999, 'epoch': 0.42}          \n",
      "{'loss': 3.9074, 'learning_rate': 0.005137500000000001, 'epoch': 0.42}          \n",
      "{'loss': 4.1501, 'learning_rate': 0.0051, 'epoch': 0.42}                        \n",
      "{'loss': 3.8747, 'learning_rate': 0.0050625, 'epoch': 0.42}                     \n",
      "{'loss': 4.0589, 'learning_rate': 0.005025, 'epoch': 0.42}                      \n",
      "{'loss': 4.3097, 'learning_rate': 0.0049875, 'epoch': 0.42}                     \n",
      "{'loss': 4.5794, 'learning_rate': 0.00495, 'epoch': 0.43}                       \n",
      "{'loss': 4.0713, 'learning_rate': 0.0049125, 'epoch': 0.43}                     \n",
      "{'loss': 4.2412, 'learning_rate': 0.004875, 'epoch': 0.43}                      \n",
      " 68%|████████████████████████▉            | 2700/4000 [1:05:37<31:41,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:25:15,794 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2700/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:25:15,795 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2700/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:25:15,807 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2700/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:25:15,808 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:25:15,808 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2700/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.9904, 'learning_rate': 0.0048375, 'epoch': 0.43}                     \n",
      "{'loss': 4.4216, 'learning_rate': 0.0048, 'epoch': 0.43}                        \n",
      "{'loss': 4.102, 'learning_rate': 0.0047625, 'epoch': 0.43}                      \n",
      "{'loss': 4.2677, 'learning_rate': 0.004725, 'epoch': 0.44}                      \n",
      "{'loss': 4.4434, 'learning_rate': 0.0046875, 'epoch': 0.44}                     \n",
      "{'loss': 4.1296, 'learning_rate': 0.00465, 'epoch': 0.44}                       \n",
      "{'loss': 4.5942, 'learning_rate': 0.004612499999999999, 'epoch': 0.44}          \n",
      "{'loss': 3.9024, 'learning_rate': 0.004575, 'epoch': 0.44}                      \n",
      "{'loss': 4.0502, 'learning_rate': 0.0045375, 'epoch': 0.44}                     \n",
      "{'loss': 4.4477, 'learning_rate': 0.0045, 'epoch': 0.45}                        \n",
      " 70%|█████████████████████████▉           | 2800/4000 [1:08:03<28:57,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:27:41,444 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2800/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:27:41,444 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2800/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:27:41,454 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2800/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:27:41,454 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:27:41,454 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2800/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2007, 'learning_rate': 0.0044624999999999995, 'epoch': 0.45}         \n",
      "{'loss': 4.0578, 'learning_rate': 0.004424999999999999, 'epoch': 0.45}          \n",
      "{'loss': 4.1358, 'learning_rate': 0.0043875, 'epoch': 0.45}                     \n",
      "{'loss': 4.2303, 'learning_rate': 0.00435, 'epoch': 0.45}                       \n",
      "{'loss': 3.9382, 'learning_rate': 0.0043124999999999995, 'epoch': 0.45}         \n",
      "{'loss': 4.0636, 'learning_rate': 0.004274999999999999, 'epoch': 0.46}          \n",
      "{'loss': 4.1556, 'learning_rate': 0.004237499999999999, 'epoch': 0.46}          \n",
      "{'loss': 4.2448, 'learning_rate': 0.004200000000000001, 'epoch': 0.46}          \n",
      "{'loss': 4.2919, 'learning_rate': 0.0041625, 'epoch': 0.46}                     \n",
      "{'loss': 3.8503, 'learning_rate': 0.004125, 'epoch': 0.46}                      \n",
      " 72%|██████████████████████████▊          | 2900/4000 [1:10:29<26:51,  1.47s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:30:07,493 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2900/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:30:07,493 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-2900/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:30:07,506 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-2900/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:30:07,506 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-2900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:30:07,507 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-2900/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.903, 'learning_rate': 0.0040875, 'epoch': 0.46}                      \n",
      "{'loss': 4.3333, 'learning_rate': 0.00405, 'epoch': 0.46}                       \n",
      "{'loss': 4.1564, 'learning_rate': 0.0040125000000000004, 'epoch': 0.47}         \n",
      "{'loss': 3.9722, 'learning_rate': 0.003975, 'epoch': 0.47}                      \n",
      "{'loss': 4.0788, 'learning_rate': 0.0039375, 'epoch': 0.47}                     \n",
      "{'loss': 4.0992, 'learning_rate': 0.0039, 'epoch': 0.47}                        \n",
      "{'loss': 4.3547, 'learning_rate': 0.0038625, 'epoch': 0.47}                     \n",
      "{'loss': 4.3564, 'learning_rate': 0.003825, 'epoch': 0.47}                      \n",
      "{'loss': 4.8181, 'learning_rate': 0.0037875, 'epoch': 0.48}                     \n",
      "{'loss': 4.1648, 'learning_rate': 0.00375, 'epoch': 0.48}                       \n",
      " 75%|███████████████████████████▊         | 3000/4000 [1:12:55<24:13,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:32:33,511 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:32:33,512 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:32:33,521 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:32:33,522 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:32:33,522 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3000/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2952, 'learning_rate': 0.0037124999999999997, 'epoch': 0.48}         \n",
      "{'loss': 4.4156, 'learning_rate': 0.003675, 'epoch': 0.48}                      \n",
      "{'loss': 4.3487, 'learning_rate': 0.0036374999999999997, 'epoch': 0.48}         \n",
      "{'loss': 4.4706, 'learning_rate': 0.0036, 'epoch': 0.48}                        \n",
      "{'loss': 3.9413, 'learning_rate': 0.0035624999999999997, 'epoch': 0.49}         \n",
      "{'loss': 4.5233, 'learning_rate': 0.0035249999999999995, 'epoch': 0.49}         \n",
      "{'loss': 4.2617, 'learning_rate': 0.0034875, 'epoch': 0.49}                     \n",
      "{'loss': 4.4034, 'learning_rate': 0.00345, 'epoch': 0.49}                       \n",
      "{'loss': 4.1019, 'learning_rate': 0.0034125, 'epoch': 0.49}                     \n",
      "{'loss': 4.3908, 'learning_rate': 0.003375, 'epoch': 0.49}                      \n",
      " 78%|████████████████████████████▋        | 3100/4000 [1:15:21<21:57,  1.46s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:34:59,420 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3100/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:34:59,421 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3100/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:34:59,431 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3100/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:34:59,431 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:34:59,431 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3100/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.0576, 'learning_rate': 0.0033374999999999998, 'epoch': 0.49}         \n",
      "{'loss': 4.1036, 'learning_rate': 0.0033, 'epoch': 0.5}                         \n",
      "{'loss': 4.144, 'learning_rate': 0.0032624999999999998, 'epoch': 0.5}           \n",
      "{'loss': 4.0336, 'learning_rate': 0.003225, 'epoch': 0.5}                       \n",
      "{'loss': 4.0632, 'learning_rate': 0.0031875, 'epoch': 0.5}                      \n",
      "{'loss': 4.1263, 'learning_rate': 0.0031499999999999996, 'epoch': 0.5}          \n",
      "{'loss': 4.3573, 'learning_rate': 0.0031125, 'epoch': 0.5}                      \n",
      "{'loss': 4.0889, 'learning_rate': 0.0030749999999999996, 'epoch': 0.51}         \n",
      "{'loss': 4.3276, 'learning_rate': 0.0030375000000000003, 'epoch': 0.51}         \n",
      "{'loss': 4.2045, 'learning_rate': 0.003, 'epoch': 0.51}                         \n",
      " 80%|█████████████████████████████▌       | 3200/4000 [1:17:46<19:17,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:37:24,624 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3200/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:37:24,624 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3200/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:37:24,634 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3200/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:37:24,634 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:37:24,634 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3200/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.2017, 'learning_rate': 0.0029625, 'epoch': 0.51}                     \n",
      "{'loss': 4.0636, 'learning_rate': 0.002925, 'epoch': 0.51}                      \n",
      "{'loss': 4.2534, 'learning_rate': 0.0028875, 'epoch': 0.51}                     \n",
      "{'loss': 4.1247, 'learning_rate': 0.00285, 'epoch': 0.52}                       \n",
      "{'loss': 4.3128, 'learning_rate': 0.0028125, 'epoch': 0.52}                     \n",
      "{'loss': 3.8753, 'learning_rate': 0.0027749999999999997, 'epoch': 0.52}         \n",
      "{'loss': 3.9336, 'learning_rate': 0.0027375, 'epoch': 0.52}                     \n",
      "{'loss': 4.419, 'learning_rate': 0.0026999999999999997, 'epoch': 0.52}          \n",
      "{'loss': 4.2104, 'learning_rate': 0.0026625, 'epoch': 0.52}                     \n",
      "{'loss': 3.9797, 'learning_rate': 0.0026249999999999997, 'epoch': 0.53}         \n",
      " 82%|██████████████████████████████▌      | 3300/4000 [1:20:11<16:48,  1.44s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:39:48,964 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3300/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:39:48,965 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3300/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:39:48,974 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3300/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:39:48,975 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:39:48,975 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3300/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.0637, 'learning_rate': 0.0025874999999999995, 'epoch': 0.53}         \n",
      "{'loss': 3.9722, 'learning_rate': 0.00255, 'epoch': 0.53}                       \n",
      "{'loss': 4.1427, 'learning_rate': 0.0025125, 'epoch': 0.53}                     \n",
      "{'loss': 4.7218, 'learning_rate': 0.002475, 'epoch': 0.53}                      \n",
      "{'loss': 4.2787, 'learning_rate': 0.0024375, 'epoch': 0.53}                     \n",
      "{'loss': 4.3861, 'learning_rate': 0.0024, 'epoch': 0.53}                        \n",
      "{'loss': 4.3591, 'learning_rate': 0.0023625, 'epoch': 0.54}                     \n",
      "{'loss': 4.207, 'learning_rate': 0.002325, 'epoch': 0.54}                       \n",
      "{'loss': 4.5177, 'learning_rate': 0.0022875, 'epoch': 0.54}                     \n",
      "{'loss': 4.4458, 'learning_rate': 0.00225, 'epoch': 0.54}                       \n",
      " 85%|███████████████████████████████▍     | 3400/4000 [1:22:35<14:22,  1.44s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:42:12,962 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3400/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:42:12,963 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3400/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:42:12,972 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3400/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:42:12,972 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:42:12,973 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3400/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.04, 'learning_rate': 0.0022124999999999996, 'epoch': 0.54}           \n",
      "{'loss': 4.3347, 'learning_rate': 0.002175, 'epoch': 0.54}                      \n",
      "{'loss': 4.1051, 'learning_rate': 0.0021374999999999996, 'epoch': 0.55}         \n",
      "{'loss': 4.0968, 'learning_rate': 0.0021000000000000003, 'epoch': 0.55}         \n",
      "{'loss': 4.4345, 'learning_rate': 0.0020625, 'epoch': 0.55}                     \n",
      "{'loss': 4.313, 'learning_rate': 0.002025, 'epoch': 0.55}                       \n",
      "{'loss': 4.267, 'learning_rate': 0.0019875, 'epoch': 0.55}                      \n",
      "{'loss': 4.1594, 'learning_rate': 0.00195, 'epoch': 0.55}                       \n",
      "{'loss': 4.3516, 'learning_rate': 0.0019125, 'epoch': 0.56}                     \n",
      "{'loss': 3.8644, 'learning_rate': 0.001875, 'epoch': 0.56}                      \n",
      " 88%|████████████████████████████████▍    | 3500/4000 [1:25:00<12:03,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:44:37,927 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:44:37,927 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3500/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:44:37,940 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:44:37,941 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:44:37,941 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3500/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.0024, 'learning_rate': 0.0018375, 'epoch': 0.56}                     \n",
      "{'loss': 4.0919, 'learning_rate': 0.0018, 'epoch': 0.56}                        \n",
      "{'loss': 4.1886, 'learning_rate': 0.0017624999999999997, 'epoch': 0.56}         \n",
      "{'loss': 4.0454, 'learning_rate': 0.001725, 'epoch': 0.56}                      \n",
      "{'loss': 4.4085, 'learning_rate': 0.0016875, 'epoch': 0.56}                     \n",
      "{'loss': 4.5513, 'learning_rate': 0.00165, 'epoch': 0.57}                       \n",
      "{'loss': 4.384, 'learning_rate': 0.0016125, 'epoch': 0.57}                      \n",
      "{'loss': 4.2378, 'learning_rate': 0.0015749999999999998, 'epoch': 0.57}         \n",
      "{'loss': 4.2954, 'learning_rate': 0.0015374999999999998, 'epoch': 0.57}         \n",
      "{'loss': 4.2194, 'learning_rate': 0.0015, 'epoch': 0.57}                        \n",
      " 90%|█████████████████████████████████▎   | 3600/4000 [1:27:25<09:38,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:47:03,052 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3600/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:47:03,052 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3600/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:47:03,064 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3600/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:47:03,064 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:47:03,064 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3600/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.7364, 'learning_rate': 0.0014625, 'epoch': 0.57}                     \n",
      "{'loss': 4.1316, 'learning_rate': 0.001425, 'epoch': 0.58}                      \n",
      "{'loss': 4.4599, 'learning_rate': 0.0013874999999999998, 'epoch': 0.58}         \n",
      "{'loss': 4.3653, 'learning_rate': 0.0013499999999999999, 'epoch': 0.58}         \n",
      "{'loss': 4.163, 'learning_rate': 0.0013124999999999999, 'epoch': 0.58}          \n",
      "{'loss': 4.2905, 'learning_rate': 0.001275, 'epoch': 0.58}                      \n",
      "{'loss': 4.1761, 'learning_rate': 0.0012375, 'epoch': 0.58}                     \n",
      "{'loss': 4.5428, 'learning_rate': 0.0012, 'epoch': 0.59}                        \n",
      "{'loss': 4.019, 'learning_rate': 0.0011625, 'epoch': 0.59}                      \n",
      "{'loss': 4.172, 'learning_rate': 0.001125, 'epoch': 0.59}                       \n",
      " 92%|██████████████████████████████████▏  | 3700/4000 [1:29:50<07:14,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:49:28,155 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3700/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:49:28,156 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3700/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:49:28,170 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3700/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:49:28,170 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:49:28,170 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3700/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.3692, 'learning_rate': 0.0010875, 'epoch': 0.59}                     \n",
      "{'loss': 4.1819, 'learning_rate': 0.0010500000000000002, 'epoch': 0.59}         \n",
      "{'loss': 3.8798, 'learning_rate': 0.0010125, 'epoch': 0.59}                     \n",
      "{'loss': 4.4624, 'learning_rate': 0.000975, 'epoch': 0.6}                       \n",
      "{'loss': 4.1141, 'learning_rate': 0.0009375, 'epoch': 0.6}                      \n",
      "{'loss': 4.2017, 'learning_rate': 0.0009, 'epoch': 0.6}                         \n",
      "{'loss': 3.9364, 'learning_rate': 0.0008625, 'epoch': 0.6}                      \n",
      "{'loss': 4.1397, 'learning_rate': 0.000825, 'epoch': 0.6}                       \n",
      "{'loss': 3.96, 'learning_rate': 0.0007874999999999999, 'epoch': 0.6}            \n",
      "{'loss': 3.6852, 'learning_rate': 0.00075, 'epoch': 0.6}                        \n",
      " 95%|███████████████████████████████████▏ | 3800/4000 [1:32:15<04:49,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:51:53,382 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3800/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:51:53,383 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3800/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:51:53,393 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3800/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:51:53,393 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:51:53,393 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3800/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.8497, 'learning_rate': 0.0007125, 'epoch': 0.61}                     \n",
      "{'loss': 3.8225, 'learning_rate': 0.0006749999999999999, 'epoch': 0.61}         \n",
      "{'loss': 4.1357, 'learning_rate': 0.0006375, 'epoch': 0.61}                     \n",
      "{'loss': 4.3795, 'learning_rate': 0.0006, 'epoch': 0.61}                        \n",
      "{'loss': 3.6649, 'learning_rate': 0.0005625, 'epoch': 0.61}                     \n",
      "{'loss': 4.0791, 'learning_rate': 0.0005250000000000001, 'epoch': 0.61}         \n",
      "{'loss': 4.2483, 'learning_rate': 0.0004875, 'epoch': 0.62}                     \n",
      "{'loss': 4.2516, 'learning_rate': 0.00045, 'epoch': 0.62}                       \n",
      "{'loss': 4.5475, 'learning_rate': 0.0004125, 'epoch': 0.62}                     \n",
      "{'loss': 4.1504, 'learning_rate': 0.000375, 'epoch': 0.62}                      \n",
      " 98%|████████████████████████████████████ | 3900/4000 [1:34:40<02:24,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:54:18,452 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3900/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:54:18,452 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-3900/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:54:18,462 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-3900/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:54:18,462 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-3900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:54:18,462 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-3900/special_tokens_map.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.123, 'learning_rate': 0.00033749999999999996, 'epoch': 0.62}         \n",
      "{'loss': 4.1488, 'learning_rate': 0.0003, 'epoch': 0.62}                        \n",
      "{'loss': 4.3515, 'learning_rate': 0.00026250000000000004, 'epoch': 0.63}        \n",
      "{'loss': 4.2776, 'learning_rate': 0.000225, 'epoch': 0.63}                      \n",
      "{'loss': 4.1438, 'learning_rate': 0.0001875, 'epoch': 0.63}                     \n",
      "{'loss': 4.1229, 'learning_rate': 0.00015, 'epoch': 0.63}                       \n",
      "{'loss': 3.9438, 'learning_rate': 0.0001125, 'epoch': 0.63}                     \n",
      "{'loss': 4.0709, 'learning_rate': 7.5e-05, 'epoch': 0.63}                       \n",
      "{'loss': 3.8383, 'learning_rate': 3.75e-05, 'epoch': 0.63}                      \n",
      "{'loss': 4.5503, 'learning_rate': 0.0, 'epoch': 0.64}                           \n",
      "100%|█████████████████████████████████████| 4000/4000 [1:37:06<00:00,  1.45s/it]Saving PrefixEncoder\n",
      "[INFO|configuration_utils.py:458] 2024-07-03 10:56:43,847 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:364] 2024-07-03 10:56:43,848 >> Configuration saved in SFT_Model_ChatGLM/checkpoint-4000/generation_config.json\n",
      "[INFO|modeling_utils.py:1853] 2024-07-03 10:56:43,861 >> Model weights saved in SFT_Model_ChatGLM/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2194] 2024-07-03 10:56:43,861 >> tokenizer config file saved in SFT_Model_ChatGLM/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2024-07-03 10:56:43,862 >> Special tokens file saved in SFT_Model_ChatGLM/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:2053] 2024-07-03 10:56:43,915 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5826.1659, 'train_samples_per_second': 2.746, 'train_steps_per_second': 0.687, 'train_loss': 4.339473876953125, 'epoch': 0.64}\n",
      "100%|█████████████████████████████████████| 4000/4000 [1:37:06<00:00,  1.46s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.64\n",
      "  train_loss               =     4.3395\n",
      "  train_runtime            = 1:37:06.16\n",
      "  train_samples            =      25140\n",
      "  train_samples_per_second =      2.746\n",
      "  train_steps_per_second   =      0.687\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=true torchrun --standalone --nnodes=1 --nproc-per-node=1 main.py \\\n",
    "    --do_train \\\n",
    "    --train_file /root/TRL2/train_mod.json \\\n",
    "    --validation_file /root/TRL2/dev_mod.json \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --prompt_column weibo \\\n",
    "    --response_column resp \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path THUDM/chatglm2-6b-int4 \\\n",
    "    --output_dir SFT_Model_ChatGLM \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 32 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps 4000 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 1.5e-2 \\\n",
    "    --pre_seq_len 128 \\\n",
    "    --report_to tensorboard \\\n",
    "    --logging_dir ./results/Final_PTuning_ChatGLM_SFT_Model\n",
    "    # --quantization_bit 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50512fd0-2d79-4452-84a9-524ddfe706bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
